{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2, *part 2* (60 points)\n",
    "\n",
    "In this assignment you will build a heavy convolutional neural net (CNN) to solve Tiny ImageNet image classification. Try to achieve as high accuracy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "* This file,\n",
    "* a \"checkpoint file\" from `torch.save(model.state_dict(), ...)` that contains model's weights (which a TA should be able to load to verify your accuracy).\n",
    "\n",
    "## Grading\n",
    "\n",
    "* 9 points for reproducible training code and a filled report below.\n",
    "* 12 points for building a network that gets above 20% accuracy.\n",
    "* 6.5 points for beating each of these milestones on the validation set:\n",
    "  * 25.0%\n",
    "  * 30.0%\n",
    "  * 32.5%\n",
    "  * 35.0%\n",
    "  * 37.5%\n",
    "  * 40.0%\n",
    "    \n",
    "## Restrictions\n",
    "\n",
    "* Don't use pretrained networks.\n",
    "\n",
    "## Tips\n",
    "\n",
    "* One change at a time: never test several new things at once.\n",
    "* Google a lot.\n",
    "* Use GPU.\n",
    "* Use regularization: L2, batch normalization, dropout, data augmentation.\n",
    "* Use Tensorboard ([non-Colab](https://github.com/lanpa/tensorboardX) or [Colab](https://medium.com/@tommytao_54597/use-tensorboard-in-google-colab-16b4bb9812a6)) or a similar interactive tool for viewing progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tiny-imagenet-200 already exists, not downloading\n"
     ]
    }
   ],
   "source": [
    "import tiny_imagenet\n",
    "tiny_imagenet.download(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation images are now in `tiny-imagenet-200/train` and `tiny-imagenet-200/val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import time, copy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM THE PYTORCH TUTORIAL (https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
    "# almost nothing was changed here (only scheduler is added)\n",
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\n",
    "    'train':torchvision.transforms.Compose([\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.RandomCrop(56),\n",
    "        torchvision.transforms.ColorJitter(saturation=.05, hue=.05),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.TenCrop(56),\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda crops: torch.stack(\n",
    "                [torchvision.transforms.ToTensor()(crop) for crop in crops]\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "}\n",
    "\n",
    "data_dir = Path('tiny-imagenet-200')\n",
    "image_datasets = {x: torchvision.datasets.ImageFolder(data_dir / x,\n",
    "                                                      transforms[x]) \n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "data_loaders = {x: data.DataLoader(image_datasets[x], \n",
    "                                   batch_size=100,\n",
    "                                   shuffle=True, \n",
    "                                   num_workers=64)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(torchvision.models.resnet18().children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunedResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.resnet18()\n",
    "        model.fc = nn.Linear(model.fc.in_features, 200) # change 1000 by 200 to fit Tiny ImageNet dataset\n",
    "        self._model = model\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if inputs.dim() > 4:\n",
    "            bs, ncrops, c, h, w = inputs.size()\n",
    "            result = self._model(inputs.view(-1, c, h, w)) \n",
    "            outputs = result.view(bs, ncrops, -1).mean(1)\n",
    "        else:\n",
    "            outputs = self._model(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TunedResNet18()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "#Multi GPU\n",
    "prll_model = nn.DataParallel(model)\n",
    "\n",
    "#Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(prll_model.parameters(), weight_decay=1e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Cyclic LR (which is not used here like periodic)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 20, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#WARNING: Don't run this cell on a weak laptop!\n",
    "#Train\n",
    "model = train_model(prll_model, \n",
    "                    data_loaders, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    num_epochs=15)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When everything is done, please compute accuracy on the validation set and report it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 49.00%\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = .49\n",
    "print(\"Validation accuracy: %.2f%%\" % (val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights in Colab and download them with the following code\n",
    "```python\n",
    "torch.save(model.state_dict(), 'model_weights-HW2.pth') \n",
    "from google.colab import files\n",
    "files.download('model_weights-HW2.pth')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEAS, USE `transforms['val']` (is is mandatory for my model)\n",
    "test_dataset = torchvision.datasets.ImageFolder(<TA, SPECIFY DIR HERE>,\n",
    "                                                transforms['val']) \n",
    "\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=10000, num_workers= <IF_NEEDED>)\n",
    "\n",
    "inputs, labels = next(iter(test_loader))\n",
    "Y_pred = model(inputs).argmax(1)\n",
    "res = (Y_pred == labels).type(torch.float).mean()\n",
    "print(\"accuracy calculated on the test :\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "Below, please mention\n",
    "\n",
    "* a brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method (batch size, optimization algorithm, ...) and why?\n",
    "* Any regularization and other techniques applied and their effects;\n",
    "\n",
    "The reference format is:\n",
    "\n",
    "*\"I have analyzed these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGS\n",
    "***A brief history of tweaks***  \n",
    "* Data augmentation\n",
    "    - apply a horizontal flip with prob. 0.5\n",
    "    - randomly crop an image of size $56 \\times 56$ pixels\n",
    "    - add a random variation in color (color jitter)\n",
    "* Validation \n",
    "    - For making a prediction on an image, first take ten crops of size $56 \\times 56$ of that image (four corners and the center, and their horizontal flips). Get the probabilities to belong to a class for each of the crops. Make a final prediction of the original image by averaging these probability vectors.\n",
    "* Architecture & Experiment Setting\n",
    "    - ResNet18 (I chose it because it has shown the best performance in the [article](http://cs231n.stanford.edu/reports/2017/pdfs/937.pdf) I followed doing this task)\n",
    "    - Adam (it performed well in HW1)\n",
    "    - L2 regularization (because it is written in the task and easy to add by passing the keyword argument to an optimizer)\n",
    "    - some other details: batch size and learning rate and the number of epochs I left unchanged (in the [code](https://github.com/tjmoon0104/Tiny-ImageNet-Classifier/blob/master/ResNet18_Baseline.ipynb) I used as a stencil) because I was doing the 2nd part of the HW in the last hours before the deadline\n",
    "    \n",
    "**Small Discussion**  \n",
    "I managed to execute \"my code\" (the code build from different snippets) two times on GPU.\n",
    "The first time I did it not using crops and the \"validation trick\" (which I took from the same [article](http://cs231n.stanford.edu/reports/2017/pdfs/937.pdf)) and got slightly higher than 40%. Having added these tweaks and run it the second time, the model did 10% better. I also was going to try cyclic learning rates from all the same [article](http://cs231n.stanford.edu/reports/2017/pdfs/937.pdf). Possibly, I will test it and download (in the case it improves the results) latter (the next day after deadline). See *After Deadline* section\n",
    "\n",
    "**REFERENCES**  \n",
    "[Code which ~~I used abusively~~ I reworked](https://github.com/tjmoon0104/Tiny-ImageNet-Classifier/blob/master/ResNet18_Baseline.ipynb) - According to my friend's words, this is the code of Stanford's similar course attendee  \n",
    "[Good article](http://cs231n.stanford.edu/reports/2017/pdfs/937.pdf) - Techniques for Image Classification on Tiny-ImageNet  \n",
    "[Favourite Documentation](https://pytorch.org/docs/stable/index.html?source=Google&medium=PaidSearch&utm_campaign=1711275050&utm_adgroup=77115349188&utm_keyword=pytorch&utm_offering=AI&utm_Product=PYTorch&gclid=EAIaIQobChMIuvy2zpTY4QIVzI4YCh15ywNTEAAYASACEgJ2y_D_BwE) - Pytorch documentation\n",
    "\n",
    "**Conclusions**  \n",
    "I have revised a lot about CNN. These things are interesting even during the second reading. I have acquired some implementation knowledge on doing data augmentation in pytorch. I have become acquainted with generous Google Colab\n",
    "\n",
    "***After Deadline***  \n",
    "++_Git commits style_++  \n",
    "- Increase the number of epochs from 15 to 20  \n",
    "(just to try if it benifits or not - no, it does not)  \n",
    "- Try CosineAnnealingLR   \n",
    "  (Cyclic LR which is here used just for obtaining the desired attenuation of LR factor)  \n",
    "  \\[a small improvement - just over 2% (48 $\\longrightarrow \\;\\gtrsim$ 50)\\]\n",
    "- Add images normalization \n",
    "  (improvement $\\sim 0.5 \\%$) - refused from it as a result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
